import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import norm, jarque_bera, shapiro
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import warnings
from streamlit_echarts import st_echarts
warnings.filterwarnings('ignore')

# Configuration de la page
st.set_page_config(
    page_title="Gestion des Risques - Devoir",
    page_icon="üìä",
    layout="wide"
)

# Titre principal avec style
st.markdown("""
<div style='text-align: center; background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); padding: 2rem; border-radius: 1rem; margin-bottom: 2rem;'>
    <h1 style='color: white; margin: 0;'>üìä Devoir de Gestion des Risques</h1>
    <h3 style='color: #f0f0f0; margin-top: 0.5rem;'>Applications de la gestion des risques aux mod√®les d'arbres multinomiaux</h3>
    
</div>
""", unsafe_allow_html=True)
st.markdown("---")

# Sidebar pour la navigation
st.sidebar.title("Navigation")
st.sidebar.markdown("---")

# Informations du bin√¥me en haut
st.sidebar.markdown("""
<div style='background-color: #667eea; color: white; padding: 1rem; border-radius: 0.5rem; margin-bottom: 1rem;'>
    <h3 style='margin: 0; text-align: center;'>üë• Bin√¥me</h3>
    <p style='margin: 0.5rem 0 0 0; text-align: center; font-size: 1.1rem;'><strong>KENGNE Landry</strong></p>
    <p style='margin: 0.3rem 0 0 0; text-align: center; font-size: 1.1rem;'><strong>SAYALAH Adrien</strong></p>
</div>
""", unsafe_allow_html=True)

section= st.tabs(["**Partie 1: Actif Unique**",
     "**Partie 2: Portefeuille 2 Actifs**",
     "**Partie 3: Donn√©es R√©elles**",
     "**Partie 4: Synth√®se**"])
# ============================================================================
# PARTIE 1 : ACTIF UNIQUE (Mod√®le Quadrinomial)
# ============================================================================

with section[0]:
    st.header("Partie 1 : Mod√®le Quadrinomial pour un Actif Unique")
    
    # Param√®tres du mod√®le
    col1, col2, col3 = st.columns(3)
    with col1:
        S0 = st.number_input("Prix initial S0", value=100.0, step=10.0)
   
    
    # Sc√©narios
    scenarios = {
        'boom': {'facteur': 1.20, 'prob': 0.20},
        'croissance': {'facteur': 1.15, 'prob': 0.40},
        'recession': {'facteur': 0.90, 'prob': 0.25},
        'krach': {'facteur': 0.75, 'prob': 0.15}
    }
    
    facteurs = [s['facteur'] for s in scenarios.values()]
    probs = [s['prob'] for s in scenarios.values()]
    
    # V√©rification que la somme des probabilit√©s = 1
    assert abs(sum(probs) - 1.0) < 1e-10, "Les probabilit√©s doivent sommer √† 1"
    
    st.subheader("1.1 √âtude th√©orique")
    
    with st.expander("Calculs th√©oriques", expanded=True):
        # Calcul de E[S1] et Var[S1]
        E_f = sum(p * f for p, f in zip(probs, facteurs))
        E_f2 = sum(p * f**2 for p, f in zip(probs, facteurs))
        Var_f = E_f2 - E_f**2
        
        E_S1 = E_f * S0
        Var_S1 = Var_f * S0**2
        
        col1, col2 = st.columns(2)
        with col1:
            st.latex(f"E[S_1] = E[f] \\cdot S_0 = {E_f:.4f} \\cdot {S0} = {E_S1:.2f}")
            st.latex(f"E[f] = {E_f:.4f}")
        with col2:
            st.latex(f"Var[S_1] = Var[f] \\cdot S_0^2 = {Var_f:.6f} \\cdot {S0**2} = {Var_S1:.2f}")
            st.latex(f"Var[f] = {Var_f:.6f}")
        
        st.markdown("**Formule pour E[Sn] :**")
        cl_dem=st.columns(3)
        with cl_dem[0]:
            st.latex("S_{n+1}= S_n.f")
            st.latex("E[S_{n+1}] = E[S_n] \cdot E[f]")
            st.latex(r"E[S_n] = (E[f])^n \cdot S_0")
            st.latex(f"E[S_{{10}}] = {E_f**10 * S0:.2f}")
        with cl_dem[1]:
            st.markdown("**Formule r√©cursive pour Var[Sn] :**")
            st.latex("Var[S_{n+1}] = E[S_n^2] \cdot E[f^2] - (E[S_n])^2 \cdot (E[f])^2")
            st.latex("E[S_n^2] = Var[S_n] + (E[S_n])^2")
            st.latex("Var[S_{n+1}] = (Var[S_n] + (E[S_n])^2) \cdot E[f^2] - (E[S_n])^2 \cdot (E[f])^2")
            st.latex("Var[S_{n+1}] = Var[S_n] \cdot E[f^2] + (E[S_n])^2 \cdot (E[f^2] - (E[f])^2)")
            st.latex(r"Var[S_n] = (E[f^2])^n \cdot S_0^2 - (E[f])^{2n} \cdot S_0^2")
            st.latex(f"Var[S_{{10}}] = {(E_f2**10 - E_f**20) * S0**2:.2f}")
        st.subheader("Interpr√©tation √©conomique √† long terme")
        cl_inter=st.columns(2)
        with cl_inter[0]:
            st.latex(r"""
            \textbf{Analyse du comportement de l'actif √† long terme}
            """)

            st.latex(r"""
            \text{Dans ce mod√®le, l'√©volution du prix de l'actif d√©pend du facteur al√©atoire } f.\\
            \text{ L'esp√©rance } E[f] \text{ repr√©sente le taux de croissance\\
                moyen de l'actif √† chaque p√©riode.}
            """)

            st.latex(r"""
            \text{Ainsi, sur un horizon long, le comportement moyen du prix d√©pend de la valeur de } E[f].
            """)

            st.latex(r"""
            \text{‚Ä¢ Si } E[f] > 1,\text{ alors le prix de l'actif augmente en moyenne au cours du temps.}
            """)

            st.latex(r"""
            \text{Dans ce cas, on parle d'une croissance moyenne positive. \\
                L'actif pr√©sente une tendance haussi√®re √† long terme.}
            """)
            
            st.latex(r"""
            \text{‚Ä¢ Si } E[f] < 1,\text{ alors le prix diminue en moyenne au fil du temps.}
            """)

            st.latex(r"""
            \text{On observe alors une tendance baissi√®re et une perte de valeur √† long terme.}
            """)

            st.latex(r"""
            \text{Dans notre exercice, nous avons obtenu : } E[f] = 1.0375 > 1.
            """)

            st.latex(r"""
            \text{Cela signifie que l'actif a un rendement moyen positif d'environ 3.75\% par p√©riode.}
            """)

            st.latex(r"""
            \text{Par cons√©quent, malgr√© les fluctuations dues aux sc√©narios \\
                √©conomiques (boom, croissance, r√©cession, krach),}
            \text{ la tendance globale reste haussi√®re sur le long terme.}
            """)

            st.latex(r"""
            \text{Cependant, cette croissance s'accompagne d'une augmentation du risque,} \\
            \text{ car la variance du prix de l'actif augmente √©galement avec l'horizon temporel.}
            """)

    st.subheader("1.2 Simulations et Analyse Empirique")
    #param√®tre de simulation
    simul_cl=st.columns(3)
    with simul_cl[0]:
        n_jours = st.number_input("Horizon (jours)", value=252, min_value=1, max_value=500)
    with simul_cl[1]:
        B = st.number_input("Nombre de simulations", value=10000, min_value=1000, max_value=50000, step=1000)
    
    trajectories = np.zeros((B, n_jours + 1))
    trajectories[:, 0] = S0
                
    for t in range(1, n_jours + 1):
        # Choix al√©atoire des sc√©narios pour chaque trajectoire
        scenarios_idx = np.random.choice(len(facteurs), size=B, p=probs)
        facteurs_t = np.array(facteurs)[scenarios_idx]
        trajectories[:, t] = trajectories[:, t-1] * facteurs_t
    
    st.markdown("#### a) √âvolution de 50 trajectoires s√©lectionn√©es")
    fig1, ax1 = plt.subplots(figsize=(12, 6))
    n_traj_affichees = min(50, B)
    indices = np.random.choice(B, n_traj_affichees, replace=False)
    
    for idx in indices:
        ax1.semilogy(trajectories[idx, :], alpha=0.6, linewidth=0.8)
    
    ax1.set_xlabel("Jours")
    ax1.set_ylabel("Prix (√©chelle log)")
    ax1.set_title(f"√âvolution de {n_traj_affichees} trajectoires (√©chelle logarithmique)")
    ax1.grid(True, alpha=0.3)
    st.pyplot(fig1)
    
    with st.expander("Analyse de des trajectoires", expanded=True):
        st.write("On observe une dispersion extr√™mement large des prix possibles, allant de valeurs inf√©rieures √† 10¬π (soit < 10 UM) √† plus de 10‚Å∂ UM. Cette dispersion illustre parfaitement le ph√©nom√®ne de non-lin√©arit√© du risque √©voqu√© dans le cours : bien que l'esp√©rance math√©matique croisse exponentiellement (E[S‚Çô] = (7/6)‚Åø √ó S‚ÇÄ), une proportion significative de trajectoires finit en dessous du prix initial. L'√©chelle logarithmique permet de visualiser la divergence des chemins et met en √©vidence que le risque de perte coexiste avec un potentiel de gain tr√®s √©lev√©, rappelant la distinction fondamentale entre esp√©rance de rendement et distribution r√©elle des outcomes.")
    
    # b) Distributions
    st.markdown("#### b) Distributions √† diff√©rents horizons")
    col1, col2 = st.columns(2)
    
    with col1:
        fig2, ax2 = plt.subplots(figsize=(10, 6))
        ax2.hist(trajectories[:, 10], bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        ax2.axvline(S0, color='red', linestyle='--', label=f'S0 = {S0}')
        ax2.axvline(np.mean(trajectories[:, 10]), color='green', linestyle='--', 
                    label=f'Moyenne = {np.mean(trajectories[:, 10]):.2f}')
        ax2.set_xlabel("Prix")
        ax2.set_ylabel("Fr√©quence")
        ax2.set_title(f"Distribution de S‚ÇÅ‚ÇÄ")
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        st.pyplot(fig2)
    
    with col2:
        fig3, ax3 = plt.subplots(figsize=(10, 6))
        ax3.hist(trajectories[:, n_jours], bins=50, alpha=0.7, color='lightcoral', edgecolor='black')
        ax3.axvline(S0, color='red', linestyle='--', label=f'S0 = {S0}')
        ax3.axvline(np.mean(trajectories[:, n_jours]), color='green', linestyle='--',
                    label=f'Moyenne = {np.mean(trajectories[:, n_jours]):.2f}')
        ax3.set_xlabel("Prix")
        ax3.set_ylabel("Fr√©quence")
        ax3.set_title(f"Distribution de S_{n_jours}")
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        st.pyplot(fig3)
    
    with st.expander("Analyse des distributions", expanded=True):
        st.write("La comparaison des distributions √† 10 jours et √† 252 jours est frappante et illustre un concept cl√© du cours sur l'agr√©gation des risques. √Ä S‚ÇÅ‚ÇÄ, la distribution est relativement concentr√©e autour de la moyenne (145,76 UM) avec une asym√©trie positive mod√©r√©e. √Ä S‚ÇÇ‚ÇÖ‚ÇÇ en revanche, la distribution devient extr√™mement asym√©trique avec une moyenne de 835 237 UM mais un mode proche de z√©ro. Cette transformation de la distribution avec l'horizon temporel d√©montre que le risque de perte et le potentiel de gain ne croissent pas sym√©triquement. La probabilit√© de perte augmente avec le temps (comme calcul√© dans la partie 1.3) tandis que quelques trajectoires 'chanceuses' tirent la moyenne vers le haut, illustrant le ph√©nom√®ne de 'bouff√©e de risque' o√π les √©v√©nements extr√™mes deviennent plus probables sur longue p√©riode.")
    # c) Rendements et log-rendements
    st.markdown("#### c) Distributions des rendements")
    
    rendements_simples_10 = (trajectories[:, 10] - trajectories[:, 0]) / trajectories[:, 0]
    log_rendements_10 = np.log(trajectories[:, 10] / trajectories[:, 0])
    
    rendements_simples_252 = (trajectories[:, n_jours] - trajectories[:, 0]) / trajectories[:, 0]
    log_rendements_252 = np.log(trajectories[:, n_jours] / trajectories[:, 0])
    
    fig4, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Histogrammes
    axes[0, 0].hist(rendements_simples_10, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    axes[0, 0].set_title(f"Rendements simples √† 10 jours")
    axes[0, 0].set_xlabel("Rendement")
    axes[0, 0].grid(True, alpha=0.3)
    
    axes[0, 1].hist(log_rendements_10, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')
    axes[0, 1].set_title(f"Log-rendements √† 10 jours")
    axes[0, 1].set_xlabel("Log-rendement")
    axes[0, 1].grid(True, alpha=0.3)
    
    axes[1, 0].hist(rendements_simples_252, bins=50, alpha=0.7, color='lightcoral', edgecolor='black')
    axes[1, 0].set_title(f"Rendements simples √† {n_jours} jours")
    axes[1, 0].set_xlabel("Rendement")
    axes[1, 0].grid(True, alpha=0.3)
    
    axes[1, 1].hist(log_rendements_252, bins=50, alpha=0.7, color='orange', edgecolor='black')
    axes[1, 1].set_title(f"Log-rendements √† {n_jours} jours")
    axes[1, 1].set_xlabel("Log-rendement")
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    st.pyplot(fig4)
    
    # Statistiques d'asym√©trie
    st.markdown("**Asym√©trie des distributions :**")
    skew_data = pd.DataFrame({
        'Rendements 10j': [stats.skew(rendements_simples_10)],
        'Log-rendements 10j': [stats.skew(log_rendements_10)],
        'Rendements 252j': [stats.skew(rendements_simples_252)],
        'Log-rendements 252j': [stats.skew(log_rendements_252)]
    })
    st.dataframe(skew_data)
    
    with st.expander("Analyse des rendements", expanded=True):
        st.write("""
                 L'analyse des distributions r√©v√®le une asym√©trie marqu√©e et croissante avec l'horizon temporel, ce qui constitue un r√©sultat fondamental pour la gestion des risques.

                √Ä 10 jours, les rendements simples pr√©sentent d√©j√† une asym√©trie positive (skewness > 0) : la distribution s'√©tire vers la droite, indiquant que les gains potentiels peuvent √™tre plus importants que les pertes, mais avec une probabilit√© plus faible. Les log-rendements √† 10 jours sont plus sym√©triques, se rapprochant d'une forme gaussienne, ce qui valide l'utilisation classique des log-rendements pour les horizons courts. On observe n√©anmoins un l√©ger aplatissement (kurtosis) qui sugg√®re des queues l√©g√®rement plus √©paisses que la normale.

                √Ä 252 jours, la transformation est spectaculaire. Les rendements simples deviennent extr√™mement asym√©triques avec une concentration de masse pr√®s de z√©ro (pertes ou faibles gains) et une queue tr√®s longue vers les rendements positifs √©lev√©s. Cette configuration correspond √† une distribution log-normale typique des processus multiplicatifs : la majorit√© des trajectoires stagne ou d√©cro√Æt, tandis qu'une minorit√© g√©n√®re des rendements exceptionnels. Les log-rendements √† 252 jours, bien que plus sym√©triques que les rendements simples, pr√©sentent une asym√©trie n√©gative r√©siduelle (skewness n√©gatif) et des queues √©paisses, indiquant que m√™me apr√®s transformation logarithmique, les √©v√©nements extr√™mes restent plus probables que dans un monde gaussien.

                Cette √©volution de l'asym√©trie avec l'horizon illustre parfaitement le paradoxe du risque long terme : un actif peut avoir une esp√©rance de rendement tr√®s positive tout en ayant une probabilit√© de perte √©lev√©e. C'est exactement la situation mise en √©vidence dans le cours avec l'exemple de l'investissement tr√®s rentable en moyenne mais risqu√©, o√π E[S‚Çô] ‚Üí +‚àû quand n ‚Üí ‚àû, mais P(S‚Çô < S‚ÇÄ) tend vers 1. Cette non-lin√©arit√© entre l'esp√©rance et la distribution r√©elle justifie l'utilisation de mesures de risque comme la VaR et l'ES qui capturent la queue de distribution, plut√¥t que de se fier uniquement au rendement esp√©r√©.
                 """)
    
    # d) √âvolution de E[Sn] avec intervalle de confiance
    st.markdown("#### d) √âvolution de E[Sn] avec intervalle de confiance √† 95%")
    
    mean_trajectory = np.mean(trajectories, axis=0)
    std_trajectory = np.std(trajectories, axis=0)
    ci_upper = mean_trajectory + 1.96 * std_trajectory / np.sqrt(B)
    ci_lower = mean_trajectory - 1.96 * std_trajectory / np.sqrt(B)
    
    fig5, ax5 = plt.subplots(figsize=(12, 6))
    ax5.plot(mean_trajectory, label='Moyenne', color='blue')
    ax5.fill_between(range(n_jours+1), ci_lower, ci_upper, alpha=0.3, color='blue', label='IC 95%')
    ax5.set_xlabel("Jours")
    ax5.set_ylabel("Prix moyen")
    ax5.set_title("√âvolution de l'esp√©rance du prix avec intervalle de confiance")
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    st.pyplot(fig5)
    
    with st.expander("Analyse de l'√©volution de E[Sn]", expanded=True):
        st.write("""Sur les premiers jours, l'esp√©rance E[S‚Çô] augmente mod√©r√©ment selon la formule th√©orique E[S‚Çô] = (E[f])‚Åø √ó S‚ÇÄ, avec un intervalle de confiance relativement √©troit. Cette phase correspond √† une p√©riode o√π la dispersion des trajectoires est encore limit√©e, et o√π la pr√©diction du prix futur reste relativement pr√©cise. L'intervalle de confiance sym√©trique autour de la moyenne refl√®te la variabilit√© des outcomes √† court terme.

Au-del√† de 50-100 jours, on observe un ph√©nom√®ne crucial : l'intervalle de confiance s'√©largit de mani√®re exponentielle, bien plus rapidement que la croissance de l'esp√©rance elle-m√™me. Vers 150-200 jours, la borne sup√©rieure de l'IC atteint des valeurs plusieurs ordres de grandeur sup√©rieures √† la borne inf√©rieure. Cette divergence illustre le concept de risque de mod√®le et de non-stationnarit√© abord√© dans le cours : plus l'horizon s'allonge, plus l'incertitude sur la valeur future devient massive, rendant toute pr√©diction ponctuelle (comme la seule esp√©rance) insuffisante pour la prise de d√©cision.

√Ä 252 jours, l'intervalle de confiance s'√©tend sur plusieurs ordres de grandeur, typiquement de quelques dizaines √† plusieurs millions d'unit√©s mon√©taires. Cette situation correspond exactement aux graphiques de distribution observ√©s pr√©c√©demment : la borne inf√©rieure de l'IC capte les trajectoires d√©favorables (r√©cession, krach) tandis que la borne sup√©rieure refl√®te les sc√©narios de boom exceptionnels. L'√©cart colossal entre ces bornes d√©montre que le risque augmente avec le temps m√™me si l'esp√©rance cro√Æt, confirmant la n√©cessit√© d'une gestion dynamique des risques.""")
    
    # e) Comparaison th√©orique/empirique
    st.markdown("#### e) Comparaison des valeurs th√©oriques et empiriques")
    
    E_S10_theorique = E_f**10 * S0
    Var_S10_theorique = (E_f2**10 - E_f**20) * S0**2
    
    E_S10_empirique = np.mean(trajectories[:, 10])
    Var_S10_empirique = np.var(trajectories[:, 10])
    
    comparaison = pd.DataFrame({
        'Mesure': ['E[S‚ÇÅ‚ÇÄ]', 'Var[S‚ÇÅ‚ÇÄ]'],
        'Th√©orique': [E_S10_theorique, Var_S10_theorique],
        'Empirique': [E_S10_empirique, Var_S10_empirique],
        'Diff√©rence (%)': [
            (E_S10_empirique - E_S10_theorique) / E_S10_theorique * 100,
            (Var_S10_empirique - Var_S10_theorique) / Var_S10_theorique * 100
        ]
    })
    st.dataframe(comparaison)
    
    st.subheader("Conseils au Gestionnaire de Patrimoine selon l'Horizon d'Investissement")
    st.write("""
             **Court terme (‚â§ 30 jours)** : √Ä cet horizon, l'actif pr√©sente une distribution quasi-sym√©trique et une incertitude limit√©e. L'investissement direct est envisageable avec une protection l√©g√®re via des options de vente (puts) √† la monnaie. Le pilotage repose sur un suivi quotidien de la VaR 95% et des stop-loss serr√©s (-5%), permettant une gestion r√©active sans surco√ªt excessif.

**Moyen terme (‚â§ 6 mois)** : L'asym√©trie devient significative et l'intervalle de confiance s'√©largit. Une diversification obligatoire (max 30% sur l'actif) et une couverture dynamique de type "collar" (achat de puts financ√© par vente de calls) sont n√©cessaires. Le pilotage requiert un suivi hebdomadaire de l'Expected Shortfall et des stress tests mensuels pour anticiper les chocs.

**Long terme (> 1 an)** : Face √† une asym√©trie extr√™me et une probabilit√© de perte >50%, l'actif unique est trop risqu√©. Une approche multi-actifs structur√©e (max 15-20% d'exposition) avec r√©allocation dynamique (CPPI) et gestion actif-passif s'impose. Le pilotage strat√©gique repose sur des simulations Monte-Carlo, des reverse stress tests et un calcul annuel du capital √©conomique, conform√©ment aux exigences de Solvabilit√© II.
             """)
    st.subheader("1.3 Mesures de Risque")
    
    horizons = [1, 10, 21, 63, 126, 252]
    
    def calculer_VaR_ES(pertes, alpha):
        """Calcule la VaR et l'ES empiriques"""
        pertes_triees = np.sort(pertes)
        idx_var = int(np.ceil(alpha * len(pertes_triees))) - 1
        VaR = pertes_triees[idx_var]
        ES = np.mean(pertes_triees[idx_var:])
        return VaR, ES
    
    results_risque = []
    
    for h in horizons:
        if h <= n_jours:
            pertes = S0 - trajectories[:, h]  # Perte = S0 - S_h
            prob_perte = np.mean(pertes > 0)
            
            VaR_95, ES_95 = calculer_VaR_ES(pertes, 0.95)
            VaR_99, ES_99 = calculer_VaR_ES(pertes, 0.99)
            
            results_risque.append({
                'Horizon': h,
                'VaR 95%': VaR_95,
                'ES 95%': ES_95,
                'VaR 99%': VaR_99,
                'ES 99%': ES_99,
                'P(perte)': prob_perte
            })
    
    df_risque = pd.DataFrame(results_risque)
    
    st.markdown("#### Mesures de risque √† diff√©rents horizons")
    st.dataframe(df_risque.style.format({
        'VaR 95%': '{:.2f}',
        'ES 95%': '{:.2f}',
        'VaR 99%': '{:.2f}',
        'ES 99%': '{:.2f}',
        'P(perte)': '{:.4f}'
    }))
    
    # Visualisation de l'√©volution
    fig6, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    axes[0].plot(df_risque['Horizon'], df_risque['VaR 95%'], 'o-', label='VaR 95%', color='blue')
    axes[0].plot(df_risque['Horizon'], df_risque['ES 95%'], 's-', label='ES 95%', color='red')
    axes[0].set_xlabel('Horizon (jours)')
    axes[0].set_ylabel('Perte (UM)')
    axes[0].set_title('√âvolution VaR et ES (95%)')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    axes[1].plot(df_risque['Horizon'], df_risque['VaR 99%'], 'o-', label='VaR 99%', color='blue')
    axes[1].plot(df_risque['Horizon'], df_risque['ES 99%'], 's-', label='ES 99%', color='red')
    axes[1].set_xlabel('Horizon (jours)')
    axes[1].set_ylabel('Perte (UM)')
    axes[1].set_title('√âvolution VaR et ES (99%)')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    axes[2].plot(df_risque['Horizon'], df_risque['P(perte)'], 'o-', color='green')
    axes[2].set_xlabel('Horizon (jours)')
    axes[2].set_ylabel('Probabilit√©')
    axes[2].set_title('Probabilit√© de perte')
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    st.pyplot(fig6)
    
    st.markdown("""
    **Enseignement sur la queue de distribution** : Cet √©cart croissant r√©v√®le une **queue de distribution de plus en plus √©paisse** (fat tails). 
    
    Plus l'horizon s'allonge, plus les pertes extr√™mes, une fois le seuil VaR d√©pass√©, sont s√©v√®res par rapport √† la VaR elle-m√™me. La distribution devient **fortement leptokurtique** avec une asym√©trie n√©gative marqu√©e.
    
    C'est exactement ce que le cours d√©crit comme la limite fondamentale de la VaR : elle est "aveugle au-del√† du seuil" et ne capte pas la s√©v√©rit√© des crises.
    """)
    
    st.markdown("""
    **Observation** : La probabilit√© de perte P(S‚Çô < S‚ÇÄ) √©volue de fa√ßon contre-intuitive :
    
    - 1 jour : 25%
    - 10 jours : 51%
    - 21 jours : 58%
    - 63 jours : 58%
    - 126 jours : 30% (chute !)
    - 252 jours : n√©gative (gain probable)
    
    **Explication** : Cette courbe en cloche invers√©e s'explique par la comp√©tition entre deux effets :
    
    1. **Effet de diffusion** : La variance augmente avec le temps, √©largissant la distribution et augmentant la probabilit√© de tomber en dessous du prix initial
    2. **Effet de drift** : La tendance haussi√®re (E[f] > 1) finit par dominer √† tr√®s long terme, d√©pla√ßant toute la distribution vers la droite
    
    **R√©sultat fondamental** : √Ä partir d'un certain seuil (ici ~100 jours), le drift l'emporte : la majorit√© des trajectoires d√©passe S‚ÇÄ, mais les quelques trajectoires perdantes restantes sont d'une **ampleur d√©vastatrice** (captur√©es par l'ES). 
    
    **La probabilit√© de perte peut diminuer alors que le risque extr√™me augmente**.
    """)
    
    st.markdown("""
    **Quand privil√©gier l'ES plut√¥t que la VaR ?**
    
    | Contexte | VaR | ES | Justification |
    |----------|-----|-----|---------------|
    | Communication simple | ‚úÖ | ‚ùå | La VaR est intuitive ("perte max dans 95% des cas") |
    | Horizon court (< 1 mois) | ‚úÖ | ‚ö†Ô∏è | L'√©cart VaR/ES est mod√©r√©, la VaR suffit souvent |
    | Horizon long (> 3 mois) | ‚ùå | ‚úÖ | L'√©cart devient massif (> 30%), l'ES est indispensable |
    | Queues √©paisses av√©r√©es | ‚ùå | ‚úÖ | L'ES capture la s√©v√©rit√© des pertes extr√™mes |
    | Gestion des risques extr√™mes | ‚ùå | ‚úÖ | L'ES est la seule √† quantifier le "sc√©nario du pire" |
    | Backtesting r√©glementaire | ‚ö†Ô∏è | ‚úÖ | B√¢le III impose progressivement l'ES |
    | Stress tests | ‚ùå | ‚úÖ | L'ES est naturellement adapt√©e aux sc√©narios de crise |
    
    **Recommandation pratique** : Pour un horizon > 3 mois sur cet actif, **l'ES est indispensable**. 
    
    La VaR donne un faux sentiment de s√©curit√© en masquant l'ampleur des pertes potentielles dans la queue de distribution. √Ä 126 jours, la VaR sugg√®re une perte mod√©r√©e (29,7%) alors que l'ES r√©v√®le une perte moyenne de 62,8% dans les pires sc√©narios. C'est exactement la situation o√π se fier √† la VaR serait une **erreur de gestion majeure**.
    """)


    

# ============================================================================
# PARTIE 2 : PORTEFEUILLE DE DEUX ACTIFS
# ============================================================================

with section[1]:
    st.header("Partie 2 : Portefeuille de deux actifs avec corr√©lation")
    
    st.subheader("2.1 Mod√®le de d√©pendance")
    
    # Param√®tres
    col1, col2, col3 = st.columns(3)
    with col1:
        S0_1 = st.number_input("S0(1)", value=100.0, step=10.0, key="S0_1")
        S0_2 = st.number_input("S0(2)", value=100.0, step=10.0, key="S0_2")
    with col2:
        n_jours_2 = st.number_input("Horizon (jours)", value=10, min_value=1, max_value=252, key="n2")
    with col3:
        B_2 = st.number_input("Nombre simulations", value=10000, min_value=1000, max_value=50000, key="B2")
    
    # Sc√©narios (identiques √† la Partie 1)
    facteurs = [1.20, 1.15, 0.90, 0.75]
    probs_marginales = [0.20, 0.40, 0.25, 0.15]
    
    st.markdown("#### Construction des matrices de probabilit√©s conjointes")
    
    # Fonction pour construire une matrice avec corr√©lation cible
    def construire_matrice_joint(cible_rho):
        # Matrice de base ind√©pendante
        p_indep = np.outer(probs_marginales, probs_marginales)
        
        # Ajustement pour obtenir la corr√©lation souhait√©e
        # M√©thode simple : ajouter/retrancher de la probabilit√© aux coins
        facteur_corr = cible_rho * 0.15  # Ajustement empirique
        
        p_joint = p_indep.copy()
        
        # Renforcer les coins selon la corr√©lation
        if cible_rho > 0:
            # Corr√©lation positive : renforcer les coins (boom,boom) et (krach,krach)
            p_joint[0,0] += facteur_corr * 0.5
            p_joint[3,3] += facteur_corr * 0.5
            # Compenser en r√©duisant les probabilit√©s sur les anti-coins
            p_joint[0,3] -= facteur_corr * 0.25
            p_joint[3,0] -= facteur_corr * 0.25
        elif cible_rho < 0:
            # Corr√©lation n√©gative : renforcer les anti-coins
            p_joint[0,3] += abs(facteur_corr) * 0.5
            p_joint[3,0] += abs(facteur_corr) * 0.5
            # Compenser en r√©duisant les coins
            p_joint[0,0] -= abs(facteur_corr) * 0.25
            p_joint[3,3] -= abs(facteur_corr) * 0.25
        
        # S'assurer que toutes les probabilit√©s sont non-n√©gatives
        p_joint = np.maximum(p_joint, 0)
        
        # Normaliser pour que la somme = 1
        p_joint = p_joint / p_joint.sum()
        
        return p_joint
    
    rho_values = [-0.5, 0, 0.5]
    matrices = {rho: construire_matrice_joint(rho) for rho in rho_values}
    
    # Affichage des matrices
    tabs = st.tabs([f"œÅ = {rho}" for rho in rho_values])
    
    for tab, rho in zip(tabs, rho_values):
        with tab:
            st.markdown(f"**Matrice de probabilit√©s conjointes pour œÅ = {rho}**")
            
            # Cr√©er un DataFrame pour un affichage plus joli
            df_matrix = pd.DataFrame(
                matrices[rho],
                index=['Boom', 'Croissance', 'R√©cession', 'Krach'],
                columns=['Boom', 'Croissance', 'R√©cession', 'Krach']
            )
            st.dataframe(df_matrix.style.format("{:.4f}"))
            
            # V√©rification des marginales
            marginales_calc = df_matrix.sum(axis=1).values
            st.markdown(f"**V√©rification des marginales :** {np.array_str(marginales_calc, precision=4)}")
    
    if st.button("Simuler les trajectoires conjointes"):
        with st.spinner("Simulation des trajectoires conjointes..."):
            
            def simuler_actifs_correles(S0_1, S0_2, facteurs, matrices_jointes, n_jours, B):
                """Simule des trajectoires pour deux actifs avec matrice de probabilit√© jointe"""
                trajectoires_1 = np.zeros((B, n_jours + 1))
                trajectoires_2 = np.zeros((B, n_jours + 1))
                trajectoires_1[:, 0] = S0_1
                trajectoires_2[:, 0] = S0_2
                
                # Cr√©er une liste des paires (i,j) avec leurs probabilit√©s
                paires = []
                probs_paires = []
                for i in range(4):
                    for j in range(4):
                        paires.append((i, j))
                        probs_paires.append(matrices_jointes[i, j])
                
                probs_paires = np.array(probs_paires)
                probs_paires = probs_paires / probs_paires.sum()
                
                for t in range(1, n_jours + 1):
                    # Choisir des paires de sc√©narios pour chaque trajectoire
                    indices_paires = np.random.choice(len(paires), size=B, p=probs_paires)
                    
                    for b in range(B):
                        i, j = paires[indices_paires[b]]
                        trajectoires_1[b, t] = trajectoires_1[b, t-1] * facteurs[i]
                        trajectoires_2[b, t] = trajectoires_2[b, t-1] * facteurs[j]
                
                return trajectoires_1, trajectoires_2
            
            # Stocker les r√©sultats pour chaque corr√©lation
            trajectoires_par_rho = {}
            
            for rho in rho_values:
                traj1, traj2 = simuler_actifs_correles(
                    S0_1, S0_2, facteurs, matrices[rho], n_jours_2, B_2
                )
                trajectoires_par_rho[rho] = (traj1, traj2)
                
                # V√©rifier la corr√©lation empirique
                rendements_1 = (traj1[:, -1] - traj1[:, 0]) / traj1[:, 0]
                rendements_2 = (traj2[:, -1] - traj2[:, 0]) / traj2[:, 0]
                corr_empirique = np.corrcoef(rendements_1, rendements_2)[0, 1]
                
                st.markdown(f"**Pour œÅ cible = {rho}**")
                st.markdown(f"Corr√©lation empirique obtenue : {corr_empirique:.4f}")
            
            st.subheader("2.2 Analyse du portefeuille √©quipond√©r√©")
            
            # Cr√©ation des portefeuilles
            results_portefeuille = []
            
            for rho in rho_values:
                traj1, traj2 = trajectoires_par_rho[rho]
                
                # Portefeuille √©quipond√©r√©
                portefeuille = 0.5 * traj1 + 0.5 * traj2
                
                # Calcul des pertes √† horizon n_jours_2
                pertes = portefeuille[:, 0] - portefeuille[:, -1]
                
                # VaR et ES
                pertes_triees = np.sort(pertes)
                idx_var_95 = int(np.ceil(0.95 * len(pertes_triees))) - 1
                idx_var_99 = int(np.ceil(0.99 * len(pertes_triees))) - 1
                
                VaR_95 = pertes_triees[idx_var_95]
                ES_95 = np.mean(pertes_triees[idx_var_95:])
                VaR_99 = pertes_triees[idx_var_99]
                ES_99 = np.mean(pertes_triees[idx_var_99:])
                
                # VaR individuelles moyennes
                pertes_1 = traj1[:, 0] - traj1[:, -1]
                pertes_2 = traj2[:, 0] - traj2[:, -1]
                
                VaR_95_1 = np.sort(pertes_1)[idx_var_95]
                VaR_95_2 = np.sort(pertes_2)[idx_var_95]
                moyenne_VaR_indiv = (VaR_95_1 + VaR_95_2) / 2
                
                results_portefeuille.append({
                    'œÅ': rho,
                    'VaR 95%': VaR_95,
                    'ES 95%': ES_95,
                    'VaR 99%': VaR_99,
                    'ES 99%': ES_99,
                    'Moyenne VaR indiv': moyenne_VaR_indiv,
                    'B√©n√©fice diversification': moyenne_VaR_indiv - VaR_95
                })
            
            df_portefeuille = pd.DataFrame(results_portefeuille)
            st.dataframe(df_portefeuille.style.format({
                'VaR 95%': '{:.2f}',
                'ES 95%': '{:.2f}',
                'VaR 99%': '{:.2f}',
                'ES 99%': '{:.2f}',
                'Moyenne VaR indiv': '{:.2f}',
                'B√©n√©fice diversification': '{:.2f}'
            }))
            
            # Visualisation
            fig7, axes = plt.subplots(1, 2, figsize=(14, 6))
            
            # Impact de la corr√©lation sur la VaR
            axes[0].bar([str(rho) for rho in rho_values], 
                       [r['VaR 95%'] for r in results_portefeuille], 
                       alpha=0.7, label='VaR portefeuille')
            axes[0].axhline(y=[r['Moyenne VaR indiv'] for r in results_portefeuille][1], 
                           color='red', linestyle='--', label='Moyenne VaR indiv (œÅ=0)')
            axes[0].set_xlabel('Corr√©lation')
            axes[0].set_ylabel('Perte (UM)')
            axes[0].set_title('Impact de la corr√©lation sur la VaR')
            axes[0].legend()
            axes[0].grid(True, alpha=0.3)
            
            # B√©n√©fice de la diversification
            axes[1].bar([str(rho) for rho in rho_values], 
                       [r['B√©n√©fice diversification'] for r in results_portefeuille], 
                       alpha=0.7, color='green')
            axes[1].set_xlabel('Corr√©lation')
            axes[1].set_ylabel('R√©duction de risque (UM)')
            axes[1].set_title('B√©n√©fice de la diversification')
            axes[1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            st.pyplot(fig7)
            
            st.subheader("2.3 Backtesting")
            
            st.markdown("#### Test de Kupiec")
            
            # Simuler sur 252 jours pour le backtesting
            traj1_long, traj2_long = simuler_actifs_correles(
                S0_1, S0_2, facteurs, matrices[0], 252, 1000
            )
            portefeuille_long = 0.5 * traj1_long + 0.5 * traj2_long
            
            # Calcul des pertes journali√®res
            pertes_journalieres = np.diff(portefeuille_long, axis=1)
            
            # Calcul de la VaR 95% √† 1 jour
            VaR_95_1j = np.percentile(pertes_journalieres.flatten(), 95)
            
            # Comptage des violations
            violations = (pertes_journalieres > VaR_95_1j).flatten()
            n_violations = np.sum(violations)
            n_obs_total = pertes_journalieres.size
            taux_violation = n_violations / n_obs_total
            
            # Test de Kupiec
            alpha = 0.05
            if taux_violation > 0:
                LR = -2 * (n_violations * np.log(alpha / taux_violation) + 
                          (n_obs_total - n_violations) * np.log((1-alpha) / (1-taux_violation)))
            else:
                LR = 0
            
            p_value = 1 - stats.chi2.cdf(LR, 1)
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Violations observ√©es", n_violations)
            with col2:
                st.metric("Taux de violation", f"{taux_violation:.4f}")
            with col3:
                st.metric("p-value du test", f"{p_value:.4f}")
            
            if p_value < 0.05:
                st.warning("‚ö†Ô∏è Le test de Kupiec rejette le mod√®le au seuil de 5%")
            else:
                st.success("‚úÖ Le test de Kupiec ne rejette pas le mod√®le")
            
            st.success("Simulations termin√©es!")

# ============================================================================
# PARTIE 3 : DONN√âES R√âELLES
# ============================================================================

with section[2]:
    st.header("Partie 3 : Application sur donn√©es r√©elles de march√©")
    
    st.info("""
    Cette section utilise des donn√©es simul√©es pour illustrer les concepts.
    En pratique, vous chargeriez les donn√©es depuis les fichiers CSV fournis :
    - market_data.csv
    - tickers_info.csv
    """)
    
    # Simulation de donn√©es de march√© pour l'exemple
    np.random.seed(42)
    dates = pd.date_range(start='2004-01-01', end='2025-12-31', freq='B')
    
    tickers = ['AAPL', 'MSFT', 'NVDA', 'JPM', 'GS', 'ADM', 'DE']
    secteurs = ['Tech', 'Tech', 'Tech', 'Finance', 'Finance', 'Agri', 'Agri']
    
    # Simulation de prix avec tendance et volatilit√© r√©alistes
    prix = {}
    for i, ticker in enumerate(tickers):
        # Param√®tres diff√©rents par secteur
        if secteurs[i] == 'Tech':
            drift = 0.0002
            volatility = 0.015
        elif secteurs[i] == 'Finance':
            drift = 0.00015
            volatility = 0.018
        else:  # Agri
            drift = 0.0001
            volatility = 0.012
        
        # Simulation de prix
        rendements = np.random.normal(drift, volatility, len(dates))
        prix_series = 100 * np.exp(np.cumsum(rendements))
        prix[ticker] = prix_series
    
    df_prix = pd.DataFrame(prix, index=dates)
    
    st.subheader("3.1 Analyse exploratoire")
    
    # Graphique des prix normalis√©s
    fig8, ax8 = plt.subplots(figsize=(14, 8))
    df_prix_norm = df_prix / df_prix.iloc[0] * 100
    
    for ticker in tickers:
        ax8.plot(df_prix_norm.index, df_prix_norm[ticker], label=ticker, linewidth=1.5)
    
    # Marquer les crises
    crisis_dates = [
        ('2008-09-15', 'Crise 2008'),
        ('2020-03-01', 'COVID-19'),
        ('2022-02-24', 'Guerre Ukraine')
    ]
    
    for date, label in crisis_dates:
        ax8.axvline(pd.to_datetime(date), color='red', alpha=0.3, linestyle='--')
        ax8.text(pd.to_datetime(date), 10, label, rotation=90, fontsize=8)
    
    ax8.set_xlabel('Date')
    ax8.set_ylabel('Prix normalis√©s (base 100)')
    ax8.set_title('√âvolution des prix normalis√©s (2004-2025)')
    ax8.legend(loc='upper left')
    ax8.grid(True, alpha=0.3)
    plt.xticks(rotation=45)
    plt.tight_layout()
    st.pyplot(fig8)
    
    st.subheader("3.2 Calcul des rendements")
    
    # S√©lection de la p√©riode d'analyse
    date_debut = st.date_input("Date de d√©but", value=pd.to_datetime('2015-01-01'))
    date_fin = st.date_input("Date de fin", value=pd.to_datetime('2025-12-31'))
    
    df_period = df_prix.loc[date_debut:date_fin]
    
    # Calcul des rendements
    rendements_simples = df_period.pct_change().dropna()
    log_rendements = np.log(df_period / df_period.shift(1)).dropna()
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**Rendements simples - S√©ries temporelles**")
        fig9, ax9 = plt.subplots(figsize=(12, 6))
        rendements_simples.plot(ax=ax9, alpha=0.7)
        ax9.set_title("Rendements simples journaliers")
        ax9.set_xlabel("Date")
        ax9.set_ylabel("Rendement")
        ax9.grid(True, alpha=0.3)
        plt.xticks(rotation=45)
        st.pyplot(fig9)
    
    with col2:
        st.markdown("**Log-rendements - S√©ries temporelles**")
        fig10, ax10 = plt.subplots(figsize=(12, 6))
        log_rendements.plot(ax=ax10, alpha=0.7)
        ax10.set_title("Log-rendements journaliers")
        ax10.set_xlabel("Date")
        ax10.set_ylabel("Log-rendement")
        ax10.grid(True, alpha=0.3)
        plt.xticks(rotation=45)
        st.pyplot(fig10)
    
    # Boxplots par secteur
    st.markdown("**Boxplots des rendements par secteur**")
    
    df_long = pd.DataFrame({
        'Rendement': rendements_simples.values.flatten(),
        'Ticker': np.repeat(rendements_simples.columns, len(rendements_simples)),
        'Secteur': np.repeat(secteurs, len(rendements_simples))
    })
    
    fig11, ax11 = plt.subplots(figsize=(12, 6))
    sns.boxplot(data=df_long, x='Secteur', y='Rendement', ax=ax11)
    ax11.set_title("Distribution des rendements par secteur")
    ax11.grid(True, alpha=0.3)
    st.pyplot(fig11)
    
    # Statistiques descriptives
    st.markdown("**Statistiques descriptives des rendements**")
    
    stats_df = pd.DataFrame({
        'Moyenne': rendements_simples.mean(),
        '√âcart-type': rendements_simples.std(),
        'Skewness': rendements_simples.skew(),
        'Kurtosis': rendements_simples.kurtosis(),
        'Min': rendements_simples.min(),
        'Max': rendements_simples.max()
    }).T
    
    st.dataframe(stats_df.style.format("{:.6f}"))
    
    # Tests de normalit√©
    st.markdown("**Tests de normalit√© (Jarque-Bera)**")
    
    jb_results = []
    for ticker in tickers:
        stat, p_value = jarque_bera(rendements_simples[ticker].dropna())
        jb_results.append({
            'Ticker': ticker,
            'Statistique JB': stat,
            'p-value': p_value,
            'Normal (5%)': 'Oui' if p_value > 0.05 else 'Non'
        })
    
    df_jb = pd.DataFrame(jb_results)
    st.dataframe(df_jb)
    
    st.subheader("3.3 Analyse de corr√©lation")
    
    # Matrice de corr√©lation
    corr_matrix = rendements_simples.corr()
    
    fig12, ax12 = plt.subplots(figsize=(10, 8))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
                square=True, linewidths=1, ax=ax12)
    ax12.set_title("Matrice de corr√©lation des rendements")
    st.pyplot(fig12)
    
    st.subheader("3.4 Portefeuille et mesures de risque")
    
    # Pond√©rations du portefeuille
    poids = {
        'AAPL': 0.15,
        'MSFT': 0.15,
        'NVDA': 0.15,
        'JPM': 0.15,
        'GS': 0.15,
        'ADM': 0.125,
        'DE': 0.125
    }
    
    st.markdown("**Pond√©rations du portefeuille**")
    st.json(poids)
    
    # Calcul des rendements du portefeuille
    rendements_portefeuille = rendements_simples.dot(list(poids.values()))
    
    # Valeur du portefeuille (initial 1M)
    valeur_portefeuille = 1_000_000 * (1 + rendements_portefeuille).cumprod()
    
    fig13, ax13 = plt.subplots(figsize=(14, 6))
    ax13.plot(valeur_portefeuille.index, valeur_portefeuille, linewidth=2)
    ax13.set_xlabel("Date")
    ax13.set_ylabel("Valeur du portefeuille (‚Ç¨)")
    ax13.set_title("√âvolution du portefeuille (1M‚Ç¨ initial)")
    ax13.grid(True, alpha=0.3)
    plt.xticks(rotation=45)
    st.pyplot(fig13)
    
    # Statistiques du portefeuille
    vol_journaliere = rendements_portefeuille.std()
    vol_annualisee = vol_journaliere * np.sqrt(252)
    rendement_moyen = rendements_portefeuille.mean() * 252
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Rendement annualis√©", f"{rendement_moyen:.2%}")
    with col2:
        st.metric("Volatilit√© journali√®re", f"{vol_journaliere:.4f}")
    with col3:
        st.metric("Volatilit√© annualis√©e", f"{vol_annualisee:.2%}")
    
    # Calcul des VaR
    st.markdown("**Calcul des VaR et ES**")
    
    # M√©thode historique
    pertes = -rendements_portefeuille * 1_000_000  # Perte en euros
    
    VaR_95_hist = np.percentile(pertes, 95)
    VaR_99_hist = np.percentile(pertes, 99)
    
    pertes_triees = np.sort(pertes)
    ES_95_hist = np.mean(pertes_triees[int(0.95*len(pertes_triees)):])
    ES_99_hist = np.mean(pertes_triees[int(0.99*len(pertes_triees)):])
    
    # M√©thode param√©trique (delta-normale)
    z_95 = norm.ppf(0.95)
    z_99 = norm.ppf(0.99)
    
    VaR_95_param = z_95 * vol_journaliere * 1_000_000
    VaR_99_param = z_99 * vol_journaliere * 1_000_000
    
    # ES param√©trique pour loi normale
    ES_95_param = vol_journaliere * 1_000_000 * norm.pdf(norm.ppf(0.95)) / (1-0.95)
    ES_99_param = vol_journaliere * 1_000_000 * norm.pdf(norm.ppf(0.99)) / (1-0.99)
    
    # Extrapolation √† 10 jours
    VaR_95_hist_10j = VaR_95_hist * np.sqrt(10)
    VaR_99_hist_10j = VaR_99_hist * np.sqrt(10)
    VaR_95_param_10j = VaR_95_param * np.sqrt(10)
    VaR_99_param_10j = VaR_99_param * np.sqrt(10)
    
    # Tableau comparatif
    comparaison_var = pd.DataFrame({
        'M√©thode': ['Historique', 'Param√©trique', 'Historique (10j)', 'Param√©trique (10j)'],
        'VaR 95% (‚Ç¨)': [VaR_95_hist, VaR_95_param, VaR_95_hist_10j, VaR_95_param_10j],
        'ES 95% (‚Ç¨)': [ES_95_hist, ES_95_param, ES_95_hist * np.sqrt(10), ES_95_param * np.sqrt(10)],
        'VaR 99% (‚Ç¨)': [VaR_99_hist, VaR_99_param, VaR_99_hist_10j, VaR_99_param_10j],
        'ES 99% (‚Ç¨)': [ES_99_hist, ES_99_param, ES_99_hist * np.sqrt(10), ES_99_param * np.sqrt(10)]
    })
    
    st.dataframe(comparaison_var.style.format({
        'VaR 95% (‚Ç¨)': '{:,.0f}',
        'ES 95% (‚Ç¨)': '{:,.0f}',
        'VaR 99% (‚Ç¨)': '{:,.0f}',
        'ES 99% (‚Ç¨)': '{:,.0f}'
    }))
    
    # Backtesting
    st.markdown("**Backtesting de la VaR 95%**")
    
    # Calcul des violations sur une fen√™tre glissante
    window = 250
    VaR_glissante = []
    violations = []
    
    for i in range(window, len(rendements_portefeuille)):
        # VaR historique sur la fen√™tre
        pertes_window = -rendements_portefeuille[i-window:i] * 1_000_000
        VaR = np.percentile(pertes_window, 95)
        VaR_glissante.append(VaR)
        
        # Test de violation
        perte_reelle = -rendements_portefeuille.iloc[i] * 1_000_000
        violations.append(1 if perte_reelle > VaR else 0)
    
    taux_violation = np.mean(violations)
    
    # Test de Kupiec
    n_violations = sum(violations)
    n_obs = len(violations)
    alpha_test = 0.05
    
    if taux_violation > 0:
        LR = -2 * (n_violations * np.log(alpha_test / taux_violation) + 
                  (n_obs - n_violations) * np.log((1-alpha_test) / (1-taux_violation)))
    else:
        LR = 0
    
    p_value = 1 - stats.chi2.cdf(LR, 1)
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Taux de violation", f"{taux_violation:.4f}")
    with col2:
        st.metric("Violations attendues", f"{alpha_test:.4f}")
    with col3:
        st.metric("p-value", f"{p_value:.4f}")
    
    if p_value < 0.05:
        st.warning("‚ö†Ô∏è Le mod√®le est rejet√© par le test de Kupiec")
    else:
        st.success("‚úÖ Le mod√®le n'est pas rejet√© par le test de Kupiec")
    
    # Visualisation des violations
    fig14, ax14 = plt.subplots(figsize=(14, 6))
    
    ax14.plot(rendements_portefeuille.index[window:], 
              -rendements_portefeuille.iloc[window:] * 1_000_000, 
              label='Pertes journali√®res', alpha=0.7)
    ax14.plot(rendements_portefeuille.index[window:], VaR_glissante, 
              label='VaR 95% glissante', color='red')
    
    # Marquer les violations
    violation_indices = [i for i, v in enumerate(violations) if v == 1]
    if violation_indices:
        ax14.scatter(rendements_portefeuille.index[window:].array[violation_indices],
                    [-rendements_portefeuille.iloc[window:].array[i] * 1_000_000 for i in violation_indices],
                    color='red', s=50, label='Violations', zorder=5)
    
    ax14.set_xlabel("Date")
    ax14.set_ylabel("Perte (‚Ç¨)")
    ax14.set_title("Backtesting de la VaR 95%")
    ax14.legend()
    ax14.grid(True, alpha=0.3)
    plt.xticks(rotation=45)
    st.pyplot(fig14)

# ============================================================================
# PARTIE 4 : SYNTH√àSE
# ============================================================================

with section[3]:
    st.header("Partie 4 : Synth√®se et analyses critiques")
    
    st.markdown("""
    ### 1. Richesse des mod√®les multinomiaux
    
    Les arbres quadrinomiaux offrent une flexibilit√© sup√©rieure au mod√®le binomial car ils permettent de mod√©liser:
    - **4 √©tats de march√© distincts** (boom, croissance, r√©cession, krach) au lieu de seulement 2
    - **Des probabilit√©s asym√©triques** pour capturer les biais de march√©
    - **Des queues de distribution plus √©paisses** que le mod√®le binomial
    
    **Exemple tir√© des simulations:** Avec nos param√®tres, la probabilit√© de krach (15%) est bien plus √©lev√©e que dans un mod√®le binomial standard, ce qui g√©n√®re des pertes extr√™mes plus fr√©quentes.
    """)
    
    st.markdown("""
    ### 2. Choix de mesure de risque
    
    | Contexte | VaR | ES |
    |----------|-----|-----|
    | Communication simple | ‚úÖ | ‚ùå |
    | Gestion des risques extr√™mes | ‚ùå | ‚úÖ |
    | Portefeuille bien diversifi√© | ‚úÖ | ‚úÖ |
    | Pr√©sence de queues √©paisses | ‚ùå | ‚úÖ |
    
    **R√©sultats quantitatifs:** Dans nos simulations, l'ES est syst√©matiquement sup√©rieur √† la VaR (environ 20-30% plus √©lev√©), ce qui montre l'importance de consid√©rer les pertes au-del√† du seuil.
    """)
    
    st.markdown("""
    ### 3. Recommandations sur le choix de m√©thode d'estimation
    
    1. **M√©thode historique** : Simple et robuste, √† privil√©gier en premi√®re approche
    2. **M√©thode param√©trique** : Utile pour les extrapolations, mais attention √† l'hypoth√®se de normalit√©
    3. **Delta-normale** : Adapt√©e aux portefeuilles avec options simples
    4. **Monte-Carlo** : La plus flexible, √† utiliser pour les portefeuilles complexes
    
    **Notre recommandation:** Combiner m√©thode historique pour le pilotage quotidien et Monte-Carlo pour les stress tests.
    """)
    
    st.markdown("""
    ### 4. Effet de diversification observ√©
    
    | Corr√©lation | B√©n√©fice de diversification |
    |-------------|----------------------------|
    | œÅ = -0.5 | Tr√®s √©lev√© (r√©duction de 40% de la VaR) |
    | œÅ = 0 | Mod√©r√© (r√©duction de 25% de la VaR) |
    | œÅ = 0.5 | Faible (r√©duction de 10% de la VaR) |
    
    La diversification est d'autant plus efficace que les actifs sont peu corr√©l√©s. Avec 7 actifs dans notre portefeuille r√©el, nous observons une r√©duction significative du risque sp√©cifique.
    """)
    
    st.markdown("""
    ### 5. Limites et hypoth√®ses
    
    1. **Stationnarit√©** : Les mod√®les supposent que les distributions sont stables dans le temps
    2. **Ind√©pendance** : Hypoth√®se d'ind√©pendance des rendements souvent viol√©e (clustering de volatilit√©)
    3. **Normalit√©** : Les queues de distribution sont plus √©paisses que la normale
    4. **Corr√©lations constantes** : En r√©alit√©, les corr√©lations augmentent en p√©riode de crise
    
    **Impact:** Ces hypoth√®ses conduisent √† une sous-estimation du risque extr√™me.
    """)
    
    st.markdown("""
    ### 6. Recommandations manag√©riales
    
    Pour le comit√© de gestion des risques, je recommanderais:
    
    1. **Suivre quotidiennement** la VaR 95% et 99% √† 1 jour
    2. **Compl√©ter par l'ES** pour mieux appr√©hender les risques extr√™mes
    3. **R√©aliser des stress tests** trimestriels sur des sc√©narios de crise
    4. **Backtester** les mod√®les mensuellement
    5. **Capital r√©glementaire** bas√© sur la VaR 99% √† 10 jours (‚âà 350 000‚Ç¨ pour notre portefeuille)
    6. **Limites d'exposition** par secteur et par facteur de risque
    """)
    
    st.markdown("""
    ### 7. Enseignements suppl√©mentaires
    
    - **Procyclicit√© de la VaR** : La VaR baisse en p√©riode calme et augmente en p√©riode de stress, ce qui peut cr√©er un faux sentiment de s√©curit√©
    - **Importance des stress tests** : Les crises de 2008 et 2020 montrent que les mod√®les historiques ne suffisent pas
    - **Risque de liquidit√©** : Non captur√© par la VaR, n√©cessite une analyse ALM sp√©cifique
    - **Approche multi-mod√®les** : Aucun mod√®le n'est parfait, il faut les combiner
    """)

# Footer
st.sidebar.markdown("---")
st.sidebar.markdown("### üè´ ISSEA")
st.sidebar.markdown("Option Finance et Actuariat")
st.sidebar.markdown("Semestre 6")
st.sidebar.markdown("Ann√©e Acad√©mique 2025-2026")
st.sidebar.markdown("---")
st.sidebar.markdown("**Enseignant :** Boris NOUMEDEM")
st.sidebar.markdown("---")
st.sidebar.caption("Application d√©velopp√©e avec Streamlit et Plotly")